{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/veera/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/veera/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/veera/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import ExcelWriter, ExcelFile\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MySQL database\n"
     ]
    }
   ],
   "source": [
    "#database connection using python\n",
    "import mysql.connector as mc\n",
    "from mysql.connector import Error\n",
    "from mysql.connector import MySQLConnection\n",
    "\n",
    "def connect():\n",
    "    \"\"\"connect to mysql database\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = mc.connect(host='localhost', database='abusive_words', user='root', password='password')\n",
    "        if conn.is_connected():\n",
    "            print('Connected to MySQL database')\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    #finally :\n",
    "     #   if conn is not None and conn.is_connected():\n",
    "      #      conn.close()\n",
    "       #     print('connection closed')\n",
    "        \n",
    "def query_with_fetchone(conn):\n",
    "    \n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"select * from abusive LIMIT 10;\")\n",
    "        row = cursor.fetchone()\n",
    "        #print(cursor.fetchone())\n",
    "        while row is not None:\n",
    "            print(row)\n",
    "            row = cursor.fetchone()\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    finally : \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "#a generator used to fetch queries size at a time, size can be 1, .... any number\n",
    "#present no need to use generator \n",
    "def iter_row(cursor, size=1):\n",
    "    while True:\n",
    "        rows = cursor.fetchmany(size)\n",
    "        if not rows:\n",
    "            break\n",
    "        for row in rows:\n",
    "            yield row\n",
    "            \n",
    "        \n",
    "def check_the_word(conn, wrd):\n",
    "    \n",
    "    if len(wrd) > 3:\n",
    "        wrd = wrd[:len(wrd)-1]\n",
    "        \n",
    "    try:\n",
    "        query = \"select * from abusive where abusive_wrds like \\'\"+wrd+\"%\\';\"\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        row = cursor.fetchone()\n",
    "        #print(row)\n",
    "        if row is None:\n",
    "            return 0, None\n",
    "        else:\n",
    "            ls = [wrd]\n",
    "            #ls.extend(list(row))\n",
    "            while row is not None:\n",
    "                ls.append(row)\n",
    "                row = cursor.fetchone()\n",
    "            \n",
    "            return 1, ls\n",
    "        \n",
    "    except Error as e:\n",
    "        print(e)\n",
    "\n",
    "#func for checking list on words in database\n",
    "def check_list(conn, wds):\n",
    "    try:\n",
    "        qry = \"select distinct * from abusive where\"\n",
    "        for w in wds:\n",
    "            qry += \" abusive_wrds like \\'\"+w+\"%\\' or\"\n",
    "        qry = qry[:-3]\n",
    "        qry += \";\"\n",
    "        print(qry)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(qry)\n",
    "        row = cursor.fetchall()\n",
    "        if row is None:\n",
    "            return 0, []\n",
    "        else:\n",
    "            return 1, [row] \n",
    "        \n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    \n",
    "    \n",
    "def close_the_connection(conn):\n",
    "    try:\n",
    "        conn.close()\n",
    "        print(\"connection is closed\")\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "\n",
    "        \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    c = connect()\n",
    "    #query_with_fetchone(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for above process\n",
    "def remove_punct(text):\n",
    "    s =set(['\"','(',')','.',',','-','<','>','/','\\',%', '\\\\x', '!','?']) #set(string.punctuation)\n",
    "    #s.remove(\"'\")\n",
    "    #s = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'\n",
    "    no_pnt = \"\".join([c for c in text if c not in s])\n",
    "    return no_pnt\n",
    "\n",
    "def tknz_text(text):\n",
    "    #lis = re.split('\\W+', text)\n",
    "    #return lis\n",
    "    #or\n",
    "    return word_tokenize(text)\n",
    "    \n",
    "#after tokenization, we have to do remove stop words\n",
    "def remove_stopwords(text):\n",
    "    stopword = nltk.corpus.stopwords.words('english')\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    #text.append()\n",
    "    return text\n",
    "\n",
    "#func for stemming --- here we use snowball stemmer over porterstemmer and lancaster stemmer\n",
    "def stmng(wrds):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    res = []\n",
    "    for w in wrds:\n",
    "        res.append(stemmer.stem(w))\n",
    "    return res \n",
    "#wordnet lemmatizer\n",
    "def lem_without(wrds):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    r = []\n",
    "    for w in wrds:\n",
    "        r.append(lemmatizer.lemmatize(w))\n",
    "    return r\n",
    "#wordnet lemmatizer with pos_tagging\n",
    "def lem_with(wrds, dic_map):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    r = []\n",
    "    for wd, tag in nltk.pos_tag(wrds):\n",
    "        r.append(lemma.lemmatize(wd, dic_map[tag[0]]))\n",
    "    return r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'remove_punct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0a1f2c32b689>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Fucking Red Neck Bitch!! who\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_punct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtknz_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'remove_punct' is not defined"
     ]
    }
   ],
   "source": [
    "#example for one sentence, similar we can do it for entire dataset by loading into pandas dataframe\n",
    "#below are some examples for abusive and normal text\n",
    "#s = \"ok but where the hell was it released?you all copy articles from each pther but did anyone do the fucking search to tell us where the fuck was it released?fuck it.\"\n",
    "#s = \"Are you kidding me?\"\n",
    "#s = \"You're a crank.\"\n",
    "#%%timeit\n",
    "#print(start_time)\n",
    "start_time = time.time()\n",
    "s = \"Fucking Red Neck Bitch!! who\"\n",
    "temp = remove_punct(s)\n",
    "print(temp)\n",
    "temp = tknz_text(temp)\n",
    "print(temp)\n",
    "temp = remove_stopwords(temp)\n",
    "print(temp)\n",
    "temp = stmng(temp) #lem_with(temp, dic_map)#stmng(temp)\n",
    "print(temp)\n",
    "c = connect()\n",
    "#checking each word in our database\n",
    "flag = 0\n",
    "\n",
    "'''for wd in temp:\n",
    "    f, word = check_the_word(c, wd)\n",
    "    if flag  == 0 and f:\n",
    "        flag = 1\n",
    "        break\n",
    "if flag == 1:\n",
    "    print(\"Abusive Detected\")\n",
    "    r = []\n",
    "    for w in word:\n",
    "        w = str(w)\n",
    "        w = w.replace(\"('\",\"\")\n",
    "        w = w.replace(\",)\", \"\")\n",
    "        r.append(w)\n",
    "    print(\"search word --------\",r[0])\n",
    "    print(\"words are \",r)\n",
    "else:\n",
    "    print(\"Normal Text\")'''\n",
    "\n",
    "fg, word = check_list(c, temp)\n",
    "if fg == 1:\n",
    "    print(\"Abusive Detected :\", word)\n",
    "else:\n",
    "    print(\"Normal Text\")\n",
    "    \n",
    "    \n",
    "    \n",
    "close_the_connection(c)\n",
    "print()\n",
    "print(time.time(), \"-\", start_time)\n",
    "print(time.time()-start_time)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
